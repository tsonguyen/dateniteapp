{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import yelp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "json_file='../yelp/yelp_academic_dataset_business.json'\n",
    "\n",
    "# read the entire file into a python array\n",
    "with open(json_file, 'rb') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# remove the trailing \"\\n\" from each line\n",
    "data = map(lambda x: x.rstrip(), data)\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "# now, load it into pandas\n",
    "bsn_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bsn_df.drop(bsn_df.columns[[0,5,9,10,14]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "json_file='../yelp/yelp_academic_dataset_review.json'\n",
    "\n",
    "# read the entire file into a python array\n",
    "with open(json_file, 'rb') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "# remove the trailing \"\\n\" from each line\n",
    "data = map(lambda x: x.rstrip(), data)\n",
    "data_json_str = \"[\" + ','.join(data) + \"]\"\n",
    "\n",
    "# now, load it into pandas\n",
    "rev_df = pd.read_json(data_json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://pandas.pydata.org/pandas-docs/stable/io.html#msgpack-experimental\n",
    "# http://stackoverflow.com/questions/17098654/how-to-store-data-frame-using-pandas-python\n",
    "bsn_df.to_pickle('business_df.pkl')\n",
    "rev_df.to_pickle('reviews_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import qgrid \n",
    "## Python packages - you may have to pip install sqlalchemy, sqlalchemy_utils, and psycopg2.\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "import psycopg2\n",
    "#bsdf = pd.read_pickle('business_df.pkl')\n",
    "#rvdf = pd.read_pickle('reviews_df.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pull out only restaurants and only in Las Vegas\n",
    "tf=[]\n",
    "for x in range(len(bsdf)):\n",
    "    tf.append('Restaurants' in bsdf['categories'][x])\n",
    "bsdf['res']=tf\n",
    "rest_bus_df=bsdf[bsdf['res']==True]\n",
    "lv_df=rest_bus_df[rest_bus_df.city=='Las Vegas']\n",
    "tot_lv_df = pd.merge(lv_df,rvdf,on='business_id',how='inner')\n",
    "# save to SQL\n",
    "dbname = 'yelp_db'\n",
    "username = 'tson'\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "print(database_exists(engine.url))\n",
    "tot_lv_df.to_sql('lv_tot_table', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter romance in comments by keyword triggers\n",
    "triggers=['my gf','my bf','significant other', 'my partner', 'my boo', 'My boo', 'My partner', 'my ex', 'My ex', 'my girlfriend','my boyfriend','my wife','my husband','my fiance','My gf','My bf','My girlfriend','My boyfriend','My wife','My husband','My fiance']\n",
    "tf=[]\n",
    "for n in range(len(tot_lv_df)):\n",
    "    tf.append(any(x in tot_lv_df['text'][n] for x in triggers))\n",
    "tot_lv_df['gfbf']=tf\n",
    "final_df=tot_lv_df[tot_lv_df['gfbf']==True]\n",
    "final_df=final_df.reset_index(drop=True)\n",
    "del final_df['categories']\n",
    "final_df.drop(final_df.columns[[14,15,16,17]], axis=1, inplace=True)\n",
    "# Save to SQL\n",
    "final_df.to_sql('lv_love_table', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filter out by category by keywords\n",
    "k1=[]\n",
    "k2=[]\n",
    "k3=[]\n",
    "k4=[]\n",
    "K1=['anniversary','our one year','our two year','our two year','our three year']\n",
    "K2=['birthday','bday']\n",
    "K3=['celebrate','congradulate','celebration','graduation']\n",
    "K4=['date night','night out']\n",
    "for n in range(len(dff)):\n",
    "    k1.append(any(x in tot_lv_df['text'][n] for x in K1))\n",
    "    k2.append(any(x in tot_lv_df['text'][n] for x in K2))\n",
    "    k3.append(any(x in tot_lv_df['text'][n] for x in K3))\n",
    "    k4.append(any(x in tot_lv_df['text'][n] for x in K4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check sentimental comments by keywords\n",
    "c1=[]\n",
    "c2=[]\n",
    "C1=['good service','excellent service','fantastic','terrific','took care','best','delicious','happy','enjoy','special','awesome']\n",
    "C2=['bad service','worst','worse','horrible','never coming back']\n",
    "for n in range(len(dff)):\n",
    "    c1.append(any(x in tot_lv_df['text'][n] for x in C1))\n",
    "    c2.append(any(x in tot_lv_df['text'][n] for x in C2))\n",
    "tot_lv_df['anniversary']=k1\n",
    "tot_lv_df['birthday']=k2\n",
    "tot_lv_df['celebration']=k3\n",
    "tot_lv_df['date_night']=k4\n",
    "tot_lv_df['pos_rev']=c1\n",
    "tot_lv_df['neg_rev']=c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate polarity of categories based on reviews\n",
    "p1=[]\n",
    "n1=[]\n",
    "p2=[]\n",
    "n2=[]\n",
    "p3=[]\n",
    "n3=[]\n",
    "p4=[]\n",
    "n4=[]\n",
    "for n in range(len(tot_lv_df)):\n",
    "    if dff['anniversary'][n]==True and tot_lv_df['pos_rev'][n]==True:\n",
    "        p1.append(tot_lv_df['stars_y'][n].astype(float))\n",
    "    if dff['anniversary'][n]==True and tot_lv_df['neg_rev'][n]==True:\n",
    "        n1.append(tot_lv_df['stars_y'][n].astype(float))\n",
    "    if dff['birthday'][n]==True and tot_lv_df['pos_rev'][n]==True:\n",
    "        p2.append(tot_lv_df['stars_y'][n].astype(float))\n",
    "    if dff['birthday'][n]==True and tot_lv_df['neg_rev'][n]==True:\n",
    "        n2.append(tot_lv_df['stars_y'][n].astype(float))\n",
    "    if dff['celebration'][n]==True and tot_lv_df['pos_rev'][n]==True:\n",
    "        p3.append(tot_lv_df['stars_y'][n].astype(float))\n",
    "    if dff['celebration'][n]==True and tot_lv_df['neg_rev'][n]==True:\n",
    "        n3.append(tot_lv_df['stars_y'][n].astype(float))\n",
    "    if dff['date_night'][n]==True and tot_lv_df['pos_rev'][n]==True:\n",
    "        p4.append(tot_lv_df['stars_y'][n].astype(float))\n",
    "    if dff['date_night'][n]==True and tot_lv_df['neg_rev'][n]==True:\n",
    "        n4.append(tot_lv_df['stars_y'][n].astype(float))\n",
    "# save to SQL\n",
    "engine = create_engine('postgres://%s@localhost/%s'%(username,dbname))\n",
    "tot_lv_df.to_sql('dates_table', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# calculate ratings of non romantic dinners\n",
    "bsdf.drop_duplicates(cols='business_id', take_last=True, inplace=True)\n",
    "bsdf.drop(bsdf.columns[[0,2,3,5,9,10,13,14]], axis=1, inplace=True)\n",
    "ddff=bsdf.sort_values('business_id')\n",
    "cn=ddff.groupby('business_id').count()\n",
    "cn.reset_index(level=0, inplace=True)\n",
    "ddff=ddff.groupby('business_id').mean()\n",
    "ddff.reset_index(level=0, inplace=True)\n",
    "ddff['num']=cn['stars_y']\n",
    "ldff=len(ddff)\n",
    "dff_cnt=pd.DataFrame()\n",
    "dff_cnt['Occasion']=bsdf.columns[[3,4,5,6]]\n",
    "dff_cnt['Total']=ddff[[3,4,5,6]].sum().values/len(dff)\n",
    "dff_cnt['Pos_Rev']=[sum(p1)/len(p1),sum(p2)/len(p2),sum(p3)/len(p3),sum(p4)/len(p4)]\n",
    "dff_cnt['Neg_Rev']=[sum(n1)/len(n1),sum(n2)/len(n2),sum(n3)/len(n3),sum(n4)/len(n4)]\n",
    "tdf = pd.merge(ddff,bff,on='business_id',how='inner')\n",
    "tdf['stars_z']=(tdf.review_count*tdf.stars-tdf.stars_y*tdf.num)/(tdf.review_count-tdf.num)\n",
    "tdf['pos_rat']=tdf['pos_rev']/(tdf['pos_rev']+tdf['neg_rev'])\n",
    "tdf['neg_rat']=tdf['neg_rev']/(tdf['pos_rev']+tdf['neg_rev'])\n",
    "# save to SQL\n",
    "tdf.to_sql('final_table', engine, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentimental Analysis via sklearn\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "def corr_matrix_of_important_words(term_doc_mat, word_list, scores, n_features_to_keep):\n",
    "    selector = SelectKBest(k=n_features_to_keep).fit(term_doc_mat, scores)\n",
    "    informative_words_index = selector.get_support(indices=True)\n",
    "    labels = [word_list[i] for i in informative_words_index]\n",
    "    data = pd.DataFrame(term_doc_mat[:,informative_words_index].todense(), columns=labels)\n",
    "    data['Score'] = scores\n",
    "    return(data.corr())\n",
    "def heat_map(corrs_mat):\n",
    "    sns.set(style=\"white\")\n",
    "    f, ax = plt.subplots(figsize=(11, 9))\n",
    "    mask = np.zeros_like(corrs_mat, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True \n",
    "    # Generate a custom diverging colormap\n",
    "    cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "    sns.heatmap(corrs_mat, mask=mask, cmap=cmap, ax=ax)\n",
    "dff = tot_lv_df\n",
    "vectorizer = CountVectorizer(max_features = 500, stop_words='english')\n",
    "term_doc_mat = vectorizer.fit_transform(dff.text)\n",
    "word_list = vectorizer.get_feature_names()\n",
    "corrs_large = corr_matrix_of_important_words(term_doc_mat, word_list, dff.stars_x, 60)\n",
    "print(corrs_large.Score.sort_values(inplace=False)[:-1])\n",
    "corrs_small = corr_matrix_of_important_words(term_doc_mat, word_list, dff.stars_x, 15)\n",
    "heat_map(corrs_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sentimental Anaylsis via TextBlob\n",
    "from __future__ import division, unicode_literals \n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "def tf(word, blob):\n",
    "    return blob.words.count(word) / len(blob.words)\n",
    "def n_containing(word, bloblist):\n",
    "    return sum(1 for blob in bloblist if word in blob)\n",
    "def idf(word, bloblist):\n",
    "    return math.log(len(bloblist) / (1 + n_containing(word, bloblist)))\n",
    "def tfidf(word, blob, bloblist):\n",
    "    return tf(word, blob) * idf(word, bloblist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dff = tot_lv_df\n",
    "scores=[]\n",
    "sorted_words=[]\n",
    "for i, blob in enumerate(bloblist):\n",
    "    #print(\"Top words in document {}\".format(i + 1))\n",
    "    scores.append({word: tfidf(word, blob, bloblist) for word in blob.words})\n",
    "    sorted_words.append(sorted(scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    #for word, score in sorted_words[:3]:\n",
    "        #print(\"\\tWord: {}, TF-IDF: {}\".format(word, round(score, 5)))\n",
    "pol=[]\n",
    "sub=[]\n",
    "for i in range(len(bloblist)):\n",
    "    pol.append(bloblist[i].sentiment.polarity)\n",
    "    sub.append(bloblist[i].sentiment.subjectivity)\n",
    "sff=pd.DataFrame()\n",
    "sff['business_id']=dff.business_id\n",
    "sff['review_id']=dff.review_id\n",
    "sff['text']=dff.text\n",
    "sff['pol']=pol\n",
    "sff['sub']=sub\n",
    "sff['stars_x']=dff.stars_x\n",
    "# save to SQL\n",
    "sff.to_sql('rev_senti_table', engine, if_exists='replace')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
